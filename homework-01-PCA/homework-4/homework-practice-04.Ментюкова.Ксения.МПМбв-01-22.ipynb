{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5943722e",
   "metadata": {},
   "source": [
    "# 1 Алгоритмы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a262b83a",
   "metadata": {},
   "source": [
    "## 1.1 Методы спуска: Общая концепция"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad46c0a6-bef2-49e5-bd05-1de6f3b090d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Итерация 0: x = [-3.  -1.8], f(x) = 12.240000, ||grad|| = 11.661904, alpha = 0.800000\n",
      "Итерация 1: x = [1.8  1.08], f(x) = 4.406400, ||grad|| = 6.997142, alpha = 0.800000\n",
      "Итерация 2: x = [-1.08  -0.648], f(x) = 1.586304, ||grad|| = 4.198285, alpha = 0.800000\n",
      "Итерация 3: x = [0.648  0.3888], f(x) = 0.571069, ||grad|| = 2.518971, alpha = 0.800000\n",
      "Итерация 4: x = [-0.3888  -0.23328], f(x) = 0.205585, ||grad|| = 1.511383, alpha = 0.800000\n",
      "Итерация 5: x = [0.23328  0.139968], f(x) = 0.074011, ||grad|| = 0.906830, alpha = 0.800000\n",
      "Итерация 6: x = [-0.139968  -0.0839808], f(x) = 0.026644, ||grad|| = 0.544098, alpha = 0.800000\n",
      "Итерация 7: x = [0.0839808  0.05038848], f(x) = 0.009592, ||grad|| = 0.326459, alpha = 0.800000\n",
      "Итерация 8: x = [-0.05038848 -0.03023309], f(x) = 0.003453, ||grad|| = 0.195875, alpha = 0.800000\n",
      "Итерация 9: x = [0.03023309 0.01813985], f(x) = 0.001243, ||grad|| = 0.117525, alpha = 0.800000\n",
      "Итерация 10: x = [-0.01813985 -0.01088391], f(x) = 0.000448, ||grad|| = 0.070515, alpha = 0.800000\n",
      "Итерация 11: x = [0.01088391 0.00653035], f(x) = 0.000161, ||grad|| = 0.042309, alpha = 0.800000\n",
      "Итерация 12: x = [-0.00653035 -0.00391821], f(x) = 0.000058, ||grad|| = 0.025385, alpha = 0.800000\n",
      "Итерация 13: x = [0.00391821 0.00235092], f(x) = 0.000021, ||grad|| = 0.015231, alpha = 0.800000\n",
      "Итерация 14: x = [-0.00235092 -0.00141055], f(x) = 0.000008, ||grad|| = 0.009139, alpha = 0.800000\n",
      "Итерация 15: x = [0.00141055 0.00084633], f(x) = 0.000003, ||grad|| = 0.005483, alpha = 0.800000\n",
      "Итерация 16: x = [-0.00084633 -0.0005078 ], f(x) = 0.000001, ||grad|| = 0.003290, alpha = 0.800000\n",
      "Итерация 17: x = [0.0005078  0.00030468], f(x) = 0.000000, ||grad|| = 0.001974, alpha = 0.800000\n",
      "Итерация 18: x = [-0.00030468 -0.00018281], f(x) = 0.000000, ||grad|| = 0.001184, alpha = 0.800000\n",
      "Итерация 19: x = [0.00018281 0.00010968], f(x) = 0.000000, ||grad|| = 0.000711, alpha = 0.800000\n",
      "Итерация 20: x = [-1.09684753e-04 -6.58108519e-05], f(x) = 0.000000, ||grad|| = 0.000426, alpha = 0.800000\n",
      "Итерация 21: x = [6.58108519e-05 3.94865112e-05], f(x) = 0.000000, ||grad|| = 0.000256, alpha = 0.800000\n",
      "Итерация 22: x = [-3.94865112e-05 -2.36919067e-05], f(x) = 0.000000, ||grad|| = 0.000153, alpha = 0.800000\n",
      "Итерация 23: x = [2.36919067e-05 1.42151440e-05], f(x) = 0.000000, ||grad|| = 0.000092, alpha = 0.800000\n",
      "Итерация 24: x = [-1.42151440e-05 -8.52908641e-06], f(x) = 0.000000, ||grad|| = 0.000055, alpha = 0.800000\n",
      "Итерация 25: x = [8.52908641e-06 5.11745185e-06], f(x) = 0.000000, ||grad|| = 0.000033, alpha = 0.800000\n",
      "Итерация 26: x = [-5.11745185e-06 -3.07047111e-06], f(x) = 0.000000, ||grad|| = 0.000020, alpha = 0.800000\n",
      "Итерация 27: x = [3.07047111e-06 1.84228266e-06], f(x) = 0.000000, ||grad|| = 0.000012, alpha = 0.800000\n",
      "Итерация 28: x = [-1.84228266e-06 -1.10536960e-06], f(x) = 0.000000, ||grad|| = 0.000007, alpha = 0.800000\n",
      "Итерация 29: x = [1.10536960e-06 6.63221759e-07], f(x) = 0.000000, ||grad|| = 0.000004, alpha = 0.800000\n",
      "Итерация 30: x = [-6.63221759e-07 -3.97933055e-07], f(x) = 0.000000, ||grad|| = 0.000003, alpha = 0.800000\n",
      "Итерация 31: x = [3.97933055e-07 2.38759833e-07], f(x) = 0.000000, ||grad|| = 0.000002, alpha = 0.800000\n",
      "Итерация 32: x = [-2.38759833e-07 -1.43255900e-07], f(x) = 0.000000, ||grad|| = 0.000001, alpha = 0.800000\n",
      "Итерация 33: x = [1.432559e-07 8.595354e-08], f(x) = 0.000000, ||grad|| = 0.000001, alpha = 0.800000\n",
      "Итерация 34: x = [-8.5953540e-08 -5.1572124e-08], f(x) = 0.000000, ||grad|| = 0.000000, alpha = 0.800000\n",
      "Итерация 35: x = [5.15721240e-08 3.09432744e-08], f(x) = 0.000000, ||grad|| = 0.000000, alpha = 0.800000\n",
      "Итерация 36: x = [-3.09432744e-08 -1.85659646e-08], f(x) = 0.000000, ||grad|| = 0.000000, alpha = 0.800000\n",
      "Итерация 37: x = [1.85659646e-08 1.11395788e-08], f(x) = 0.000000, ||grad|| = 0.000000, alpha = 0.800000\n",
      "Итерация 38: x = [-1.11395788e-08 -6.68374727e-09], f(x) = 0.000000, ||grad|| = 0.000000, alpha = 0.800000\n",
      "Итерация 39: x = [6.68374727e-09 4.01024836e-09], f(x) = 0.000000, ||grad|| = 0.000000, alpha = 0.800000\n",
      "Итерация 40: x = [-4.01024836e-09 -2.40614902e-09], f(x) = 0.000000, ||grad|| = 0.000000, alpha = 0.800000\n",
      "Алгоритм сошелся на итерации 41. Норма градиента: 0.000000\n",
      "\n",
      "Результат оптимизации:\n",
      "Найденная точка: [-4.01024836e-09 -2.40614902e-09]\n",
      "Значение функции: 0.0000000000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gradient_descent(f, grad_f, x0, max_iter=100, tol=1e-6, alpha0=1.0, beta=0.5, c=1e-4):\n",
    "    \"\"\"\n",
    "    Общая реализация метода спуска\n",
    "    \n",
    "    Параметры:\n",
    "    f -- оптимизируемая функция\n",
    "    grad_f -- функция вычисления градиента f\n",
    "    x0 -- начальная точка (numpy массив)\n",
    "    max_iter -- максимальное число итераций\n",
    "    tol -- порог для критерия остановки по норме градиента\n",
    "    alpha0 -- начальное значение шага для линейного поиска\n",
    "    beta -- коэффициент уменьшения шага (0 < beta < 1)\n",
    "    c -- параметр условия Армихо (0 < c < 1)\n",
    "    \n",
    "    Возвращает:\n",
    "    x -- найденная точка минимума\n",
    "    \"\"\"\n",
    "    x = np.array(x0, dtype=float)\n",
    "    \n",
    "    for k in range(max_iter):\n",
    "        # Шаг 2: Вычисление значения функции и градиента (оракул)\n",
    "        grad = grad_f(x)\n",
    "        current_f = f(x)\n",
    "        \n",
    "        # Шаг 3: Критерий остановки (проверка нормы градиента)\n",
    "        if np.linalg.norm(grad) < tol:\n",
    "            print(f\"Алгоритм сошелся на итерации {k}. Норма градиента: {np.linalg.norm(grad):.6f}\")\n",
    "            break\n",
    "        \n",
    "        # Шаг 4: Вычисление направления спуска (градиентный спуск)\n",
    "        d_k = -grad\n",
    "        \n",
    "        # Шаг 5: Линейный поиск (условие Армихо)\n",
    "        alpha = alpha0\n",
    "        while f(x + alpha * d_k) > current_f + c * alpha * np.dot(grad, d_k):\n",
    "            alpha *= beta\n",
    "        \n",
    "        # Шаг 6: Обновление точки\n",
    "        x = x + alpha * d_k\n",
    "        \n",
    "        # Отладочная информация\n",
    "        print(f\"Итерация {k}: x = {x}, f(x) = {f(x):.6f}, ||grad|| = {np.linalg.norm(grad):.6f}, alpha = {alpha:.6f}\")\n",
    "    \n",
    "    return x\n",
    "\n",
    "# Пример использования\n",
    "if __name__ == \"__main__\":\n",
    "    # Тестируем на квадратичной функции f(x) = x_1^2 + x_2^2\n",
    "    def objective_function(x):\n",
    "        return np.sum(x**2)\n",
    "    \n",
    "    def gradient(x):\n",
    "        return 2 * x\n",
    "    \n",
    "    # Начальная точка\n",
    "    x0 = np.array([5.0, 3.0])\n",
    "    \n",
    "    # Запуск алгоритма\n",
    "    result = gradient_descent(\n",
    "        f=objective_function,\n",
    "        grad_f=gradient,\n",
    "        x0=x0,\n",
    "        max_iter=50,\n",
    "        tol=1e-8,\n",
    "        alpha0=1.0,\n",
    "        beta=0.8,\n",
    "        c=1e-4\n",
    "    )\n",
    "    \n",
    "    print(\"\\nРезультат оптимизации:\")\n",
    "    print(f\"Найденная точка: {result}\")\n",
    "    print(f\"Значение функции: {objective_function(result):.10f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e831bc55",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1.2 Критерий остановки\n",
    "\n",
    "Идеальным критерием остановки в методе является проверка условия $$f(x_k)−f^*< \\tilde{ε}$$, где $f^*$ - минимальное значение функции $f$, а $\\tilde{ε} > 0$ - заданная точность. Такой критерий целесообразно использовать, если оптимальное значение функции $f$ известно. К сожалению, зачастую это не так, и поэтому нужно использовать другой критерий. Наиболее популярным является критерий, основанный на норме градиента: $$‖∇f(x_k)‖^2_2 <\\tilde{ε}$$. Квадрат здесь ставят за тем, что для \"хороших\" функций невязка по функции $f(x_k)−f^*$ имеет тот же порядок, что и $‖∇f(x_k)‖^2_2$ , а не $‖∇f(x_k)‖_2$ (например, это верно для сильно-выпуклых функций с липшицевым градиентом.); например, если $‖∇f(x_k)‖_2 ∼ 10^{−5}$, то $f(x_k)−f^* ∼ 10^{−10}$. Наконец, для того, чтобы критерий не зависел от того, измеряется ли функция $f$ в \"метрах\" или в \"километрах\" (т. е. не изменялся при переходе от функции $f$ к функции $tf$, где $t > 0$), то имеет смысл использовать следующий относительный вариант критерия:\n",
    "$$ ‖∇f(x_k)‖^2_2 ≤ ε‖∇f(x_0)‖^2_2 \\tag{1.1},$$\n",
    "где $ε∈(0,1)$ - заданная относительнаяточность. Таким образом, критерий остановки (1.1) гарантирует, что метод уменьшит начальную невязку $‖∇f(x_0)‖_2$ в $ε^{−1}$ раз. В этом задании Вам нужно будет во всех методах использовать критерий остановки (1.1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3758836a",
   "metadata": {},
   "source": [
    "## 1.3 Линейный поиск"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1f1c340-7542-4888-b2b3-340cf6c4296a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Градиентный спуск с линейным поиском (условие Армихо)\n",
      "==================================================\n",
      "Итерация   0: α = 5.00e-01, f(x) = 1.8000e+01, ||∇f|| = 1.56e+01\n",
      "Итерация   1: α = 2.50e-01, f(x) = 0.0000e+00, ||∇f|| = 1.20e+01\n",
      "Алгоритм сошелся на итерации 2. Норма градиента: 0.00e+00\n",
      "\n",
      "==================================================\n",
      "Градиентный спуск с линейным поиском (сильные условия Вульфа)\n",
      "==================================================\n",
      "Итерация   0: α = 3.14e-01, f(x) = 4.6392e+00, ||∇f|| = 1.56e+01\n",
      "Итерация   1: α = 3.55e-01, f(x) = 5.0051e-01, ||∇f|| = 4.83e+00\n",
      "Итерация   2: α = 3.14e-01, f(x) = 5.3999e-02, ||∇f|| = 1.69e+00\n",
      "Итерация   3: α = 3.55e-01, f(x) = 5.8258e-03, ||∇f|| = 5.21e-01\n",
      "Итерация   4: α = 3.14e-01, f(x) = 6.2854e-04, ||∇f|| = 1.82e-01\n",
      "Итерация   5: α = 3.55e-01, f(x) = 6.7811e-05, ||∇f|| = 5.62e-02\n",
      "Итерация   6: α = 3.14e-01, f(x) = 7.3160e-06, ||∇f|| = 1.96e-02\n",
      "Итерация   7: α = 3.55e-01, f(x) = 7.8931e-07, ||∇f|| = 6.07e-03\n",
      "Итерация   8: α = 3.14e-01, f(x) = 8.5157e-08, ||∇f|| = 2.12e-03\n",
      "Итерация   9: α = 3.55e-01, f(x) = 9.1874e-09, ||∇f|| = 6.55e-04\n",
      "Итерация  10: α = 3.14e-01, f(x) = 9.9121e-10, ||∇f|| = 2.28e-04\n",
      "Итерация  11: α = 3.55e-01, f(x) = 1.0694e-10, ||∇f|| = 7.06e-05\n",
      "Итерация  12: α = 3.14e-01, f(x) = 1.1537e-11, ||∇f|| = 2.46e-05\n",
      "Итерация  13: α = 3.55e-01, f(x) = 1.2447e-12, ||∇f|| = 7.62e-06\n",
      "Итерация  14: α = 3.14e-01, f(x) = 1.3429e-13, ||∇f|| = 2.66e-06\n",
      "Итерация  15: α = 3.55e-01, f(x) = 1.4489e-14, ||∇f|| = 8.22e-07\n",
      "Итерация  16: α = 3.14e-01, f(x) = 1.5631e-15, ||∇f|| = 2.87e-07\n",
      "Итерация  17: α = 3.55e-01, f(x) = 1.6864e-16, ||∇f|| = 8.87e-08\n",
      "Итерация  18: α = 3.14e-01, f(x) = 1.8195e-17, ||∇f|| = 3.09e-08\n",
      "Алгоритм сошелся на итерации 19. Норма градиента: 9.57e-09\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import line_search\n",
    "\n",
    "def armijo_line_search(f, x, d, grad, alpha_prev, c1=1e-4, adaptive=True, max_trials=100):\n",
    "    \"\"\"Линейный поиск с условием Армихо и адаптивным шагом\"\"\"\n",
    "    alpha = alpha_prev if adaptive else 1.0\n",
    "    phi0 = f(x)\n",
    "    phi_prime_0 = np.dot(grad, d)\n",
    "    \n",
    "    # Проверка, что направление является направлением спуска\n",
    "    if phi_prime_0 >= 0:\n",
    "        raise ValueError(\"Направление не является направлением спуска: ∇f·d ≥ 0\")\n",
    "    \n",
    "    for _ in range(max_trials):\n",
    "        new_x = x + alpha * d\n",
    "        if f(new_x) <= phi0 + c1 * alpha * phi_prime_0:\n",
    "            next_alpha_prev = 2 * alpha if adaptive else 1.0\n",
    "            return alpha, next_alpha_prev\n",
    "        alpha *= 0.5\n",
    "        if alpha < 1e-15:  # Защита от слишком маленьких шагов\n",
    "            break\n",
    "    \n",
    "    # Если условие не выполнено - возвращаем последний alpha\n",
    "    next_alpha_prev = 2 * alpha if adaptive else 1.0\n",
    "    return alpha, next_alpha_prev\n",
    "\n",
    "def wolfe_line_search(f, grad_f, x, d, c1=1e-4, c2=0.9):\n",
    "    \"\"\"Линейный поиск по сильным условиям Вульфа с использованием scipy.optimize.line_search\"\"\"\n",
    "    # Вычисляем текущее значение функции и градиента\n",
    "    phi0 = f(x)\n",
    "    grad0 = grad_f(x)\n",
    "    \n",
    "    # Проверка направления спуска\n",
    "    if np.dot(grad0, d) >= 0:\n",
    "        raise ValueError(\"Направление не является направлением спуска: ∇f·d ≥ 0\")\n",
    "    \n",
    "    try:\n",
    "        # Используем линейный поиск из SciPy\n",
    "        result = line_search(\n",
    "            f=f,\n",
    "            myfprime=grad_f,\n",
    "            xk=x,\n",
    "            pk=d,\n",
    "            gfk=grad0,\n",
    "            old_fval=phi0,\n",
    "            c1=c1,\n",
    "            c2=c2,\n",
    "            amax=100.0  # Максимальное значение alpha для поиска\n",
    "        )\n",
    "        \n",
    "        alpha, fc, gc, new_fval, old_fval, new_slope = result\n",
    "        \n",
    "        # Проверяем корректность найденного шага\n",
    "        if alpha is None or alpha <= 0:\n",
    "            print(f\"Предупреждение: line_search вернул некорректный шаг alpha={alpha}. Используем резервное значение 1.0\")\n",
    "            alpha = 1.0\n",
    "        elif np.isnan(alpha):\n",
    "            print(\"Предупреждение: line_search вернул NaN. Используем резервное значение 1.0\")\n",
    "            alpha = 1.0\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Предупреждение: ошибка линейного поиска Wolfe: {e}. Используем резервное значение 1.0\")\n",
    "        alpha = 1.0\n",
    "    \n",
    "    return alpha\n",
    "\n",
    "def gradient_descent(f, grad_f, x0, max_iter=100, tol=1e-8, \n",
    "                    line_search_method='armijo', \n",
    "                    c1=1e-4, c2=0.9, \n",
    "                    alpha0=1.0, adaptive=True):\n",
    "    \"\"\"\n",
    "    Градиентный спуск с различными методами линейного поиска\n",
    "    \n",
    "    Параметры:\n",
    "    line_search_method: 'armijo' или 'wolfe'\n",
    "    adaptive: Использовать адаптивный начальный шаг (только для 'armijo')\n",
    "    \"\"\"\n",
    "    x = np.array(x0, dtype=float)\n",
    "    alpha_prev = alpha0\n",
    "    history = []\n",
    "    \n",
    "    for k in range(max_iter):\n",
    "        grad = grad_f(x)\n",
    "        grad_norm = np.linalg.norm(grad)\n",
    "        current_f = f(x)\n",
    "        history.append((k, current_f, grad_norm))\n",
    "        \n",
    "        # Критерий остановки\n",
    "        if grad_norm < tol:\n",
    "            print(f\"Алгоритм сошелся на итерации {k}. Норма градиента: {grad_norm:.2e}\")\n",
    "            break\n",
    "        \n",
    "        # Направление спуска (градиентный спуск)\n",
    "        d = -grad\n",
    "        \n",
    "        # Линейный поиск\n",
    "        if line_search_method == 'armijo':\n",
    "            alpha, alpha_prev = armijo_line_search(\n",
    "                f, x, d, grad, alpha_prev,\n",
    "                c1=c1, adaptive=adaptive\n",
    "            )\n",
    "        elif line_search_method == 'wolfe':\n",
    "            alpha = wolfe_line_search(\n",
    "                f, grad_f, x, d,\n",
    "                c1=c1, c2=c2\n",
    "            )\n",
    "            # Для методов Вульфа всегда начинаем поиск со значения 1.0\n",
    "            alpha_prev = 1.0\n",
    "        else:\n",
    "            raise ValueError(\"Неизвестный метод линейного поиска. Используйте 'armijo' или 'wolfe'\")\n",
    "        \n",
    "        # Обновление точки\n",
    "        x = x + alpha * d\n",
    "        \n",
    "        # Отладочная информация\n",
    "        print(f\"Итерация {k:3d}: α = {alpha:.2e}, f(x) = {f(x):.4e}, ||∇f|| = {grad_norm:.2e}\")\n",
    "    \n",
    "    return x, history\n",
    "\n",
    "# Пример использования\n",
    "if __name__ == \"__main__\":\n",
    "    # Тестируем на квадратичной функции f(x) = x_1^2 + 2*x_2^2\n",
    "    def objective_function(x):\n",
    "        return x[0]**2 + 2*x[1]**2\n",
    "    \n",
    "    def gradient(x):\n",
    "        return np.array([2*x[0], 4*x[1]])\n",
    "    \n",
    "    # Начальная точка\n",
    "    x0 = np.array([5.0, 3.0])\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    print(\"Градиентный спуск с линейным поиском (условие Армихо)\")\n",
    "    print(\"=\"*50)\n",
    "    result_armijo, hist_armijo = gradient_descent(\n",
    "        f=objective_function,\n",
    "        grad_f=gradient,\n",
    "        x0=x0,\n",
    "        max_iter=50,\n",
    "        line_search_method='armijo',\n",
    "        adaptive=True,\n",
    "        alpha0=1.0\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Градиентный спуск с линейным поиском (сильные условия Вульфа)\")\n",
    "    print(\"=\"*50)\n",
    "    result_wolfe, hist_wolfe = gradient_descent(\n",
    "        f=objective_function,\n",
    "        grad_f=gradient,\n",
    "        x0=x0,\n",
    "        max_iter=50,\n",
    "        line_search_method='wolfe'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b1bbf4",
   "metadata": {},
   "source": [
    "## 1.4 Градиентный спуск\n",
    "\n",
    "Градиентный спуск:\n",
    "$$x_{k+1}=x_k−α_k∇f(x_k)$$\n",
    "Можно рассматривать как метод спуска, в котором направление поиска $d_k$ равно антиградиенту\n",
    "$−∇f(x_k)$. Длина шага $α_k$ выбирается с помощью линейного поиска."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afeb4d7",
   "metadata": {},
   "source": [
    "## 1.5 Метод Ньютона\n",
    "\n",
    "Метод Ньютона:\n",
    "$$x_{k+1}=x_k−α_k[∇^2 f(x_k)]^{-1} ∇f(x_k).$$\n",
    "Для метода Ньютона очень важно использовать единичный шаг $α_k = 1$, чтобы обеспечить локальную квадратичную сходимость. Поэтому в алгоритмах линейного поиска нужно всегда первым делом\n",
    "пробовать единичный шаг. Теория гарантирует, что в зоне квадратичной сходимости метода Ньютона\n",
    "единичный шаг будет удовлетворять условиям Армихо/Вульфа, и поэтому автоматически будет приниматься. Если единичный шаг не удовлетворяет условиям Армихо/Вульфа, то алгоритмы линейного\n",
    "поиска его уменьшат и, тем самым, обеспечат глобальную сходимость метода Ньютона.  \n",
    "\n",
    "Вычисление Ньютоновского направления $d_k=−[∇^2 f(x_k)]^{-1} ∇f(x_k)$ эквивалентно решению линей-\n",
    "ной системы уравнений:\n",
    "$$∇^2 f(x_k)d_k=−∇f(x_k).$$\n",
    "Если гессиан - положительно определённая матрица: $∇^2 f(x_k) \\succ 0$ , то предпочтительным методом решения такой системы является разложение Холецкого, которое также, как и метод Гаусса, работает за $O(n^3)$, но является вычислительно более эффективным. Если матрица системы не является положительно определённой, то метод Холецкого сможет обнаружить и сообщить об этом."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ed864b",
   "metadata": {},
   "source": [
    "## 1.6 (Бонусная часть) Оптимизация вычислений\n",
    "\n",
    "```\n",
    "Рассмотрим случайf(x) =ψ(Ax).\n",
    "В этом случае\n",
    "∇f(x) =AT∇ψ(Ax).\n",
    "```\n",
    "Для линейного поиска:\n",
    "\n",
    "```\n",
    "φ(α) =ψ(Axk+αAdk), φ′(α) =〈∇ψ(Axk+αAdk),Adk〉.\n",
    "```\n",
    "Алгоритм 3Общая схема метода спуска дляf(x) =ψ(Ax)\n",
    "\n",
    "```\n",
    "1:fork← 0 toK− 1 do\n",
    "2: (Вызов оракула)Вычислитьf(xk) =ψ(Axk),∇f(xk) =AT∇ψ(Axk)и пр.\n",
    "3: (Вычисление направления)Вычислить направление спускаdk.\n",
    "4: (Линейный поиск)Найти подходящую длину шагаαk:\n",
    "5: Вычислитьφ(0) =ψ(Axk),φ′(0) =〈∇ψ(Axk),Adk〉.\n",
    "6: Вычислитьφ( ̄α 1 ) =ψ(Axk+ ̄α 1 Adk),φ′( ̄α 1 ) =〈∇ψ(Axk+ ̄α 1 Adk),Adk〉.\n",
    "7: ...\n",
    "8: Вычислитьφ( ̄αs) =ψ(Axk+ ̄αsAdk),φ′( ̄αs) =〈∇ψ(Axk+ ̄αsAdk),Adk〉.\n",
    "9: (Обновление)xk+1←xk+ ̄αsdk.. Axk+1=Axk+ ̄αsAdk\n",
    "10:end for\n",
    "```\n",
    "Таким образом, в хорошей реализации должно быть в среднем лишь дваматрично-векторных про-\n",
    "изведения: одно \u0016 чтобы вычислить градиентAT∇ψ(Axk), второе \u0016 чтобы вычислитьAdk. Сами\n",
    "матрично-векторные произведенияAxkможно пересчитывать, используя Adk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2d7e0c4-5094-4cc7-885e-57119a4a1d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Оптимизированный градиентный спуск с условием Армихо\n",
      "============================================================\n",
      "Итерация   0: α = 5.00e-01, f(x) = 1.8000e+01, ||∇f|| = 1.56e+01\n",
      "Итерация   1: α = 2.50e-01, f(x) = 0.0000e+00, ||∇f|| = 1.20e+01\n",
      "Алгоритм сошелся на итерации 2. Норма градиента: 0.00e+00\n",
      "\n",
      "============================================================\n",
      "Оптимизированный градиентный спуск с сильными условиями Вульфа\n",
      "============================================================\n",
      "Итерация   0: α = 5.00e-01, f(x) = 1.8000e+01, ||∇f|| = 1.56e+01\n",
      "Итерация   1: α = 2.50e-01, f(x) = 0.0000e+00, ||∇f|| = 1.20e+01\n",
      "Алгоритм сошелся на итерации 2. Норма градиента: 0.00e+00\n",
      "\n",
      "============================================================\n",
      "Сравнение результатов:\n",
      "Армихо: x = [ 0.0000000e+00 -4.4408921e-16], f(x) = 0.000000\n",
      "Вульф : x = [ 0.0000000e+00 -4.4408921e-16], f(x) = 0.000000\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def armijo_line_search_opt(psi, Ax, Ad, phi0, phi_prime_0, alpha_prev, \n",
    "                          c1=1e-4, adaptive=True, max_trials=100):\n",
    "    \"\"\"Оптимизированный линейный поиск с условием Армихо, использующий предвычисленные Ax и Ad\"\"\"\n",
    "    alpha = alpha_prev if adaptive else 1.0\n",
    "    for _ in range(max_trials):\n",
    "        new_Ax = Ax + alpha * Ad\n",
    "        phi_alpha = psi(new_Ax)\n",
    "        if phi_alpha <= phi0 + c1 * alpha * phi_prime_0:\n",
    "            next_alpha_prev = 2 * alpha if adaptive else 1.0\n",
    "            return alpha, next_alpha_prev, new_Ax\n",
    "        alpha *= 0.5\n",
    "        if alpha < 1e-15:\n",
    "            break\n",
    "    \n",
    "    # Если условие не выполнено - возвращаем последний alpha\n",
    "    new_Ax = Ax + alpha * Ad\n",
    "    next_alpha_prev = 2 * alpha if adaptive else 1.0\n",
    "    return alpha, next_alpha_prev, new_Ax\n",
    "\n",
    "def wolfe_line_search_opt(psi, grad_psi, Ax, Ad, phi0, phi_prime_0, \n",
    "                         c1=1e-4, c2=0.9, max_trials=100):\n",
    "    \"\"\"Оптимизированный линейный поиск по сильным условиям Вульфа\"\"\"\n",
    "    alpha = 1.0\n",
    "    for _ in range(max_trials):\n",
    "        new_Ax = Ax + alpha * Ad\n",
    "        phi_alpha = psi(new_Ax)\n",
    "        grad_psi_val = grad_psi(new_Ax)\n",
    "        phi_prime_alpha = np.dot(grad_psi_val, Ad)\n",
    "        \n",
    "        # Проверка условий Вульфа\n",
    "        armijo_condition = (phi_alpha <= phi0 + c1 * alpha * phi_prime_0)\n",
    "        curvature_condition = (abs(phi_prime_alpha) <= c2 * abs(phi_prime_0))\n",
    "        \n",
    "        if armijo_condition and curvature_condition:\n",
    "            return alpha, new_Ax\n",
    "        \n",
    "        # Если условия не выполнены, уменьшаем alpha\n",
    "        alpha *= 0.5\n",
    "        if alpha < 1e-15:\n",
    "            break\n",
    "    \n",
    "    # Возвращаем последнее значение\n",
    "    new_Ax = Ax + alpha * Ad\n",
    "    return alpha, new_Ax\n",
    "\n",
    "def gradient_descent_optimized(A, psi, grad_psi, x0, max_iter=100, tol=1e-8, \n",
    "                              line_search_method='armijo', \n",
    "                              c1=1e-4, c2=0.9, \n",
    "                              alpha0=1.0, adaptive=True):\n",
    "    \"\"\"\n",
    "    Градиентный спуск с оптимизацией вычислений для функции вида f(x) = ψ(Ax)\n",
    "    \n",
    "    Параметры:\n",
    "    A -- матрица преобразования (n x m)\n",
    "    psi -- функция ψ: R^m -> R\n",
    "    grad_psi -- градиент функции ψ\n",
    "    x0 -- начальная точка (размер n)\n",
    "    \"\"\"\n",
    "    x = np.array(x0, dtype=float)\n",
    "    Ax = A @ x  # Первое матрично-векторное произведение\n",
    "    alpha_prev = alpha0\n",
    "    history = []\n",
    "    \n",
    "    for k in range(max_iter):\n",
    "        # Шаг 2: Вычисление градиента\n",
    "        grad_psi_val = grad_psi(Ax)\n",
    "        grad_f = A.T @ grad_psi_val  # Второе матрично-векторное произведение\n",
    "        grad_norm = norm(grad_f)\n",
    "        current_f = psi(Ax)\n",
    "        history.append((k, current_f, grad_norm))\n",
    "        \n",
    "        # Критерий остановки\n",
    "        if grad_norm < tol:\n",
    "            print(f\"Алгоритм сошелся на итерации {k}. Норма градиента: {grad_norm:.2e}\")\n",
    "            break\n",
    "        \n",
    "        # Шаг 3: Направление спуска (градиентный спуск)\n",
    "        d = -grad_f\n",
    "        \n",
    "        # Шаг 4: Вычисление Ad\n",
    "        Ad = A @ d   # В описании подразумевается, что A @ d вычисляется отдельно,\n",
    "                    # Ax обновляется через Ax + alpha * Ad\n",
    "        phi_prime_0 = np.dot(grad_psi_val, Ad)  # ∇ψ(Ax)^T @ Ad\n",
    "        \n",
    "        # Проверка направления спуска\n",
    "        if phi_prime_0 >= 0:\n",
    "            raise ValueError(f\"Направление не является спуском: ∇f·d = {phi_prime_0:.2e} >= 0\")\n",
    "        \n",
    "        # Шаг 5: Линейный поиск\n",
    "        if line_search_method == 'armijo':\n",
    "            alpha, alpha_prev, new_Ax = armijo_line_search_opt(\n",
    "                psi, Ax, Ad, current_f, phi_prime_0, alpha_prev,\n",
    "                c1=c1, adaptive=adaptive\n",
    "            )\n",
    "        elif line_search_method == 'wolfe':\n",
    "            alpha, new_Ax = wolfe_line_search_opt(\n",
    "                psi, grad_psi, Ax, Ad, current_f, phi_prime_0,\n",
    "                c1=c1, c2=c2\n",
    "            )\n",
    "            alpha_prev = 1.0  # Для Вульфа всегда начинаем с 1.0\n",
    "        else:\n",
    "            raise ValueError(\"Неизвестный метод линейного поиска. Используйте 'armijo' или 'wolfe'\")\n",
    "        \n",
    "        # Шаг 6: Обновление x и Ax\n",
    "        x = x + alpha * d\n",
    "        Ax = new_Ax  # Обновление без матрично-векторного умножения!\n",
    "        \n",
    "        # Отладочная информация\n",
    "        print(f\"Итерация {k:3d}: α = {alpha:.2e}, f(x) = {psi(Ax):.4e}, ||∇f|| = {grad_norm:.2e}\")\n",
    "    \n",
    "    return x, history\n",
    "\n",
    "# Пример использования\n",
    "if __name__ == \"__main__\":\n",
    "    # Матрица для функции f(x) = x₁² + 2x₂² = ||Ax||², где A = [[1, 0], [0, √2]]\n",
    "    A = np.array([[1.0, 0.0],\n",
    "                  [0.0, np.sqrt(2.0)]])\n",
    "    \n",
    "    # Функция ψ(y) = ||y||² и её градиент\n",
    "    def psi(y):\n",
    "        return np.sum(y**2)\n",
    "    \n",
    "    def grad_psi(y):\n",
    "        return 2 * y\n",
    "    \n",
    "    # Начальная точка\n",
    "    x0 = np.array([5.0, 3.0])\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"Оптимизированный градиентный спуск с условием Армихо\")\n",
    "    print(\"=\"*60)\n",
    "    result_armijo, hist_armijo = gradient_descent_optimized(\n",
    "        A=A,\n",
    "        psi=psi,\n",
    "        grad_psi=grad_psi,\n",
    "        x0=x0,\n",
    "        max_iter=50,\n",
    "        line_search_method='armijo',\n",
    "        adaptive=True,\n",
    "        alpha0=1.0\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Оптимизированный градиентный спуск с сильными условиями Вульфа\")\n",
    "    print(\"=\"*60)\n",
    "    result_wolfe, hist_wolfe = gradient_descent_optimized(\n",
    "        A=A,\n",
    "        psi=psi,\n",
    "        grad_psi=grad_psi,\n",
    "        x0=x0,\n",
    "        max_iter=50,\n",
    "        line_search_method='wolfe'\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Сравнение результатов:\")\n",
    "    print(f\"Армихо: x = {result_armijo}, f(x) = {psi(A @ result_armijo):.6f}\")\n",
    "    print(f\"Вульф : x = {result_wolfe}, f(x) = {psi(A @ result_wolfe):.6f}\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eeb48c6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 2 Модели\n",
    "\n",
    "## 2.1 Двухклассовая логистическая регрессия\n",
    "\n",
    "Логистическая регрессия является стандартной моделью в задачах классификации. Для простоты\n",
    "рассмотрим лишь случай бинарной классификации. Неформально задача формулируется следующим\n",
    "образом. Имеется обучающая выборка $((a_i, b_i))^m_{i=1}$, состоящая изmвекторов $a_i ∈ R^n$ (называемых признаками) и соответствующих им чисел $b_i ∈ {−1, 1}$ (называемых классами). Нужно построить алгоритм $b(·)$, который для произвольного нового вектора признаков $a$ автоматически определит его класс $b(a)∈{−1, 1}$.  \n",
    "\n",
    "В модели логистической регрессии определение класса выполняется по знаку линейной комбинации\n",
    "компонент вектораaс некоторыми фиксированными коэффициентами $x∈R^n$:\n",
    "$$b(a) := sign(〈a,x〉).$$\n",
    "\n",
    "Коэффициенты $x$ являются параметрами модели и настраиваются с помощью решения следующей\n",
    "оптимизационной задачи:\n",
    "$$\\underset{x∈R^n}{min} \\left( \\frac{1}{m}\\sum_{i=1}^m ln(1 + exp(−b_i〈a_i, x〉)) + \\frac{λ}{2}‖x‖^2_2 \\right) $$\n",
    "где $λ > 0$ - коэффициент регуляризации (параметр модели).\n",
    "\n",
    "## 2.2 Разностная проверка градиента и гессиана\n",
    "Проверить правильность реализации подсчета градиента можно с помощью конечных разностей:\n",
    "$$[∇f(x)]_i ≈ \\frac{f(x+ε_1 e_i)−f(x)}{ε_1},$$\n",
    "где $e_i:= (0,..., 0 , 1 , 0 ,...,0)$ - i-й базисный орт, а ε_1 - достаточно маленькое положительное число: $ε_1 ∼ \\sqrt{ε_{mach}}$, где $ε_{mach}$ - машинная точность ($≈ 10 ^{-16}$ для типа `double`).\n",
    "\n",
    "Вторые производные:\n",
    "$$[∇^2 f(x)]_{ij} ≈ \\frac{f(x + ε_2 e_i + ε_2 e_j) − f(x + ε_2 e_i) − f(x + ε_2 e_j) + f(x)}{ε^2_2}$$\n",
    "Здесь $ε_2 ∼\\sqrt[3]{ε_{mach}}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3979d0da-1af7-4086-ae4b-75484758117f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер обучающей выборки: 105, тестовой: 45\n",
      "\n",
      "Примеры автоматического определения класса для новых данных:\n",
      "Формат: [признак1, признак2] -> предсказанный_класс (истинный_класс)\n",
      "[-0.225, -0.177] -> -1 (-1.0)\n",
      "[-0.033, 2.066] -> 1 (1.0)\n",
      "[-1.924, 0.367] -> -1 (-1.0)\n",
      "[1.117, -0.784] -> 1 (1.0)\n",
      "[1.240, -0.473] -> 1 (1.0)\n",
      "[-1.058, 0.421] -> -1 (-1.0)\n",
      "[-0.243, 1.000] -> 1 (1.0)\n",
      "[-0.480, -1.364] -> -1 (-1.0)\n",
      "[1.231, 0.629] -> 1 (1.0)\n",
      "[0.807, -0.963] -> 1 (-1.0)\n",
      "\n",
      "Точность автоматического определения класса на новых данных: 0.978\n",
      "Точность на обучающей выборке: 0.933\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from scipy.special import expit\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, lambda_reg=0.1, max_iter=1000, tol=1e-4):\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.coef_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        m, n = X.shape\n",
    "        x0 = np.zeros(n)\n",
    "        \n",
    "        def loss(x):\n",
    "            linear = X @ x\n",
    "            log_terms = np.logaddexp(0, -y * linear)\n",
    "            avg_log_loss = np.mean(log_terms)\n",
    "            reg_term = (self.lambda_reg / 2) * np.dot(x, x)\n",
    "            return avg_log_loss + reg_term\n",
    "        \n",
    "        def grad(x):\n",
    "            linear = X @ x\n",
    "            sigma = expit(-y * linear)\n",
    "            grad_data = - (X.T @ (y * sigma)) / m\n",
    "            grad_reg = self.lambda_reg * x\n",
    "            return grad_data + grad_reg\n",
    "        \n",
    "        res = minimize(\n",
    "            fun=loss,\n",
    "            x0=x0,\n",
    "            jac=grad,\n",
    "            method='L-BFGS-B',\n",
    "            options={'maxiter': self.max_iter, 'gtol': self.tol}\n",
    "        )\n",
    "        self.coef_ = res.x\n",
    "\n",
    "    def predict(self, X):\n",
    "        linear = X @ self.coef_\n",
    "        return np.where(linear >= 0, 1, -1)\n",
    "\n",
    "# Генерация\n",
    "X_full = np.random.randn(150, 2)\n",
    "y_full = np.sign(X_full[:, 0] + X_full[:, 1] + np.random.randn(150) * 0.2)\n",
    "\n",
    "# Разделение на обучающую и тестовую выборки\n",
    "split_idx = int(0.7 * len(X_full))\n",
    "X_train, X_test = X_full[:split_idx], X_full[split_idx:]\n",
    "y_train, y_test = y_full[:split_idx], y_full[split_idx:]\n",
    "\n",
    "print(f\"Размер обучающей выборки: {len(X_train)}, тестовой: {len(X_test)}\")\n",
    "\n",
    "# Обучение модели\n",
    "model = LogisticRegression(lambda_reg=0.1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Автоматическое определение класса для новых (тестовых) данных\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Вывод примеров автоматического определения класса\n",
    "print(\"\\nПримеры автоматического определения класса для новых данных:\")\n",
    "print(\"Формат: [признак1, признак2] -> предсказанный_класс (истинный_класс)\")\n",
    "for i in range(min(10, len(X_test))):\n",
    "    features = X_test[i]\n",
    "    predicted_class = predictions[i]\n",
    "    true_class = y_test[i]\n",
    "    print(f\"[{features[0]:.3f}, {features[1]:.3f}] -> {predicted_class} ({true_class})\")\n",
    "\n",
    "# Оценка точности на новых данных\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "print(f\"\\nТочность автоматического определения класса на новых данных: {accuracy:.3f}\")\n",
    "\n",
    "# Проверка, что модель не просто запоминает обучающие данные\n",
    "train_predictions = model.predict(X_train)\n",
    "train_accuracy = np.mean(train_predictions == y_train)\n",
    "print(f\"Точность на обучающей выборке: {train_accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5a0bb1e-2c94-4681-b2fe-cc00e57dacab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## 2.2 Разностная проверка градиента и гессиана"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca7e59a7-eba6-4cef-b04b-8b6f0a8191c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Проверка градиента:\n",
      "  Норма разности: 2.43e-08\n",
      "  Относительная ошибка: 6.70e-08\n",
      "\n",
      "Проверка гессиана:\n",
      "  Норма разности: 2.65e-05\n",
      "  Относительная ошибка: 5.35e-05\n",
      "\n",
      "Результаты:\n",
      "  Градиент ✓ ПРОЙДЕН (порог: 1e-06)\n",
      "  Гессиан ✓ ПРОЙДЕН (порог: 0.0001)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "\n",
    "def logistic_loss(x, X, y, lambda_reg):\n",
    "    \"\"\"Функция потерь логистической регрессии.\"\"\"\n",
    "    m = X.shape[0]\n",
    "    linear = X @ x\n",
    "    log_terms = np.logaddexp(0, -y * linear)\n",
    "    avg_log_loss = np.mean(log_terms)\n",
    "    reg_term = (lambda_reg / 2) * np.dot(x, x)\n",
    "    return avg_log_loss + reg_term\n",
    "\n",
    "def logistic_gradient(x, X, y, lambda_reg):\n",
    "    \"\"\"Градиент функции потерь.\"\"\"\n",
    "    m = X.shape[0]\n",
    "    linear = X @ x\n",
    "    sigma = expit(-y * linear)\n",
    "    grad_data = - (X.T @ (y * sigma)) / m\n",
    "    grad_reg = lambda_reg * x\n",
    "    return grad_data + grad_reg\n",
    "\n",
    "def logistic_hessian(x, X, y, lambda_reg):\n",
    "    \"\"\"Аналитический гессиан функции потерь.\"\"\"\n",
    "    m = X.shape[0]\n",
    "    linear = X @ x\n",
    "    # sigma = 1 / (1 + exp(y * linear)) = expit(-y * linear)\n",
    "    sigma = expit(-y * linear)\n",
    "    # w_i = sigma_i * (1 - sigma_i)\n",
    "    w = sigma * (1 - sigma)\n",
    "    # Гессиан: (1/m) * X.T @ diag(w) @ X + lambda_reg * I\n",
    "    hess_data = (X.T * w) @ X / m\n",
    "    hess_reg = lambda_reg * np.eye(len(x))\n",
    "    return hess_data + hess_reg\n",
    "\n",
    "def numerical_gradient(x, X, y, lambda_reg, eps=None):\n",
    "    \"\"\"Численный градиент с помощью конечных разностей.\"\"\"\n",
    "    if eps is None:\n",
    "        eps = np.sqrt(np.finfo(float).eps)  # ~1e-8\n",
    "    n = len(x)\n",
    "    grad_num = np.zeros(n)\n",
    "    f0 = logistic_loss(x, X, y, lambda_reg)\n",
    "    for i in range(n):\n",
    "        x_perturbed = x.copy()\n",
    "        x_perturbed[i] += eps\n",
    "        f1 = logistic_loss(x_perturbed, X, y, lambda_reg)\n",
    "        grad_num[i] = (f1 - f0) / eps\n",
    "    return grad_num\n",
    "\n",
    "def numerical_hessian(x, X, y, lambda_reg, eps=None):\n",
    "    \"\"\"Численный гессиан с помощью конечных разностей второго порядка.\"\"\"\n",
    "    if eps is None:\n",
    "        eps = np.cbrt(np.finfo(float).eps)  # ~1e-5\n",
    "    n = len(x)\n",
    "    hess_num = np.zeros((n, n))\n",
    "    f0 = logistic_loss(x, X, y, lambda_reg)\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i == j:\n",
    "                # Вторая производная по одной переменной\n",
    "                x_pp = x.copy()\n",
    "                x_pp[i] += eps\n",
    "                f_pp = logistic_loss(x_pp, X, y, lambda_reg)\n",
    "                \n",
    "                x_mm = x.copy()\n",
    "                x_mm[i] -= eps\n",
    "                f_mm = logistic_loss(x_mm, X, y, lambda_reg)\n",
    "                \n",
    "                hess_num[i, j] = (f_pp - 2*f0 + f_mm) / (eps**2)\n",
    "            elif i < j:\n",
    "                # Смешанная производная\n",
    "                x_ij = x.copy()\n",
    "                x_ij[i] += eps\n",
    "                x_ij[j] += eps\n",
    "                f_ij = logistic_loss(x_ij, X, y, lambda_reg)\n",
    "                \n",
    "                x_i = x.copy()\n",
    "                x_i[i] += eps\n",
    "                f_i = logistic_loss(x_i, X, y, lambda_reg)\n",
    "                \n",
    "                x_j = x.copy()\n",
    "                x_j[j] += eps\n",
    "                f_j = logistic_loss(x_j, X, y, lambda_reg)\n",
    "                \n",
    "                hess_num[i, j] = (f_ij - f_i - f_j + f0) / (eps**2)\n",
    "                hess_num[j, i] = hess_num[i, j]  # симметрия\n",
    "    return hess_num\n",
    "\n",
    "def check_gradient_and_hessian():\n",
    "    \"\"\"Проверка градиента и гессиана.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    m, n = 50, 5\n",
    "    X = np.random.randn(m, n)\n",
    "    y = np.random.choice([-1, 1], size=m)\n",
    "    lambda_reg = 0.1\n",
    "    \n",
    "    # Случайная точка x\n",
    "    x = np.random.randn(n)\n",
    "    \n",
    "    # Аналитические производные\n",
    "    grad_analytic = logistic_gradient(x, X, y, lambda_reg)\n",
    "    hess_analytic = logistic_hessian(x, X, y, lambda_reg)\n",
    "    \n",
    "    # Численные производные\n",
    "    grad_numeric = numerical_gradient(x, X, y, lambda_reg)\n",
    "    hess_numeric = numerical_hessian(x, X, y, lambda_reg)\n",
    "    \n",
    "    # Проверка градиента\n",
    "    grad_diff = np.linalg.norm(grad_analytic - grad_numeric)\n",
    "    grad_norm = np.linalg.norm(grad_analytic)\n",
    "    print(f\"Проверка градиента:\")\n",
    "    print(f\"  Норма разности: {grad_diff:.2e}\")\n",
    "    print(f\"  Относительная ошибка: {grad_diff / (grad_norm + 1e-12):.2e}\")\n",
    "    \n",
    "    # Проверка гессиана\n",
    "    hess_diff = np.linalg.norm(hess_analytic - hess_numeric)\n",
    "    hess_norm = np.linalg.norm(hess_analytic)\n",
    "    print(f\"\\nПроверка гессиана:\")\n",
    "    print(f\"  Норма разности: {hess_diff:.2e}\")\n",
    "    print(f\"  Относительная ошибка: {hess_diff / (hess_norm + 1e-12):.2e}\")\n",
    "    \n",
    "    # Порог для успешной проверки\n",
    "    tol_grad = 1e-6\n",
    "    tol_hess = 1e-4\n",
    "    grad_ok = grad_diff < tol_grad\n",
    "    hess_ok = hess_diff < tol_hess\n",
    "    \n",
    "    print(f\"\\nРезультаты:\")\n",
    "    print(f\"  Градиент {'✓ ПРОЙДЕН' if grad_ok else '✗ НЕ ПРОЙДЕН'} (порог: {tol_grad})\")\n",
    "    print(f\"  Гессиан {'✓ ПРОЙДЕН' if hess_ok else '✗ НЕ ПРОЙДЕН'} (порог: {tol_hess})\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    check_gradient_and_hessian()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e7fe3e",
   "metadata": {},
   "source": [
    "# 3 Формулировка задания\n",
    "\n",
    "1 Скачайте коды, прилагаемые к заданию:\n",
    "\n",
    "https://github.com/arodomanov/cmc-mipt17-opt-course/tree/master/task\n",
    "\n",
    "Эти файлы содержат прототипы функций, которые Вам нужно будет реализовать. Некоторые проце-\n",
    "дуры уже частично или полностью реализованы.\n",
    "\n",
    "2 Реализовать метод градиентного спуска (функция `gradient_descent` в модуле `optimization`) и процедуру линейного поиска (метод `line_search` в классе `LineSearchTool` в модуле `optimization`).  \n",
    "**Рекомендация:** Для поиска точки, удовлетворяющей сильным условиям Вульфа, воспользуйтесь биб-\n",
    "лиотечной функцией `scalar_search_wolfe2` из модуля `scipy.optimize.linesearch`. Однако следует\n",
    "иметь в виду, что у этой библиотечной функции имеется один недостаток: она иногда не сходится и\n",
    "возвращает значение `None`. Если библиотечный метод вернул `None`, то запустите процедуру дробления шага (бэктрекинг) для поиска точки, удовлетворяющей условию Армихо.\n",
    "\n",
    "3 Получить формулы для градиента и гессиана функции логистической регрессии. Выписать их в отчет\n",
    "в матрично-векторной форме с использованием поэлементных функций, но без каких-либо суммирований. Также выписать в отчетвыражение для самой функции логистической регрессии в матрично-векторной форме (без явных суммирований).  \n",
    "**Замечание:** В матрично-вектрной форме допускается использование операций матричного сложения/произведения, умножения на скаляр, транспонирования, стандартного скалярного произведения, поэлементного произведения, а также применения ко всем элементам вектора некоторой скалярной функции. Кроме этого, допускается использование стандартных матриц/векторов (заданного размера): единичная матрица $I_n$, нулевая матрица $0_{m×n}$, нулевой вектор $0_n$, вектор из всех единиц $1_n := (1,... ,1)$.\n",
    "\n",
    "4 Реализовать оракул логистической регрессии (класс `LogRegL2Oracle` в модуле `oracles`). Также доделать реализацию вспомогательной функции `create_log_reg_oracle` в модуле `oracles`.  \n",
    "**Замечание:** Реализация оракула должна быть полностью векторизованной, т. е. код не должен содержать никаких циклов.  \n",
    "**Замечание:** Ваш код должен поддерживать как плотные матрицыAтипаnp.array, так и разрежен-\n",
    "ные типа `scipy.sparse.csr_matrix`.  \n",
    "**Замечание:** Нигде в промежуточных вычислениях не стоит вычислять значение $exp(−b_i〈a_i, x〉)$, иначе может произойти переполнение. Вместо этого следует напрямую вычислять необходимые величины с помощью специализированных для этого функций: `np.logaddexp` для $ln(1+exp(·))$ и `scipy.special.expit` для $1 /(1 + exp(·))$.\n",
    "\n",
    "5 Реализовать подсчет разностных производных (функции `grad_finite_diff` и `hess_finite_diff` в модуле `oracles`). Проверить правильность реализации подсчета градиента и гессиана логистического\n",
    "оракула с помощью реализованных функций. Для этого сгенерируйте небольшую модельную выборку\n",
    "(матрицу $A$ и вектор $b$) и сравните значения, выдаваемые методами `grad` и `hess`, с соответствующими разностными аппроксимациями в нескольких пробных точкахx.\n",
    "\n",
    "6 Реализовать метод Ньютона (функция `newton` в модуле `optimization`).\n",
    "\n",
    "**Замечание:** Для поиска направления в методе Ньютона не нужно в явном виде обращать гессиан (с\n",
    "помощью функции `np.linalg.inv`) или использовать самый общий метод для решения системы линей-\n",
    "ных уравнений (`numpy.linalg.solve`). Вместо этого следует учесть тот факт, что в рассматриваемой\n",
    "задаче гессиан является симметричной положительно определенной матрицей и воспользоваться раз-\n",
    "ложением Холецкого (функции `scipy.linalg.cho_factor` и `scipy.linalg.cho_solve`).\n",
    "\n",
    "7 Провести эксперименты, описанные ниже. Написать отчет.\n",
    "\n",
    "<!-- 8 (Бонусная часть) Реализовать оптимизированный оракул логистической регрессии, который запомина-\n",
    "ет последние матрично-векторные произведения (классLogRegL2OptimizedOracleв модулеoptimization).\n",
    "Оптимизированный оракул отличается от обычного в следующих трех пунктах:\n",
    "\n",
    "1. При последовательных вычислениях значения функции (методfunc), градиента (методgrad) и\n",
    "    гессиана (методhess) в одной и той же точкеx, матрично-векторное произведениеAxне вычис-\n",
    "    ляется повторно.\n",
    "2. В процедурахfunc_directionalиgrad_directionalвыполняется предподсчет матрично-векторных\n",
    "    произведенийAxиAd. Если эти процедуры вызываются последовательно для одних и тех же зна-\n",
    "    чений точкиxи/или направленияd, то матрично-векторные произведенияAxи/илиAdзаново не\n",
    "    вычисляются. Если перед вызовом или после вызоваfunc_directionalи/илиgrad_directional\n",
    "    присутствуют вызовыfuncи/илиgradи/илиhessв той же самой точкеx, то матрично-векторное\n",
    "    произведениеAxне должно вычисляться повторно.\n",
    "3. Методыfunc_directionalиgrad_directionalзапоминают внутри себя последнюю тестовую\n",
    "    точкуxˆ:=x+αd, а также соответствующее значение матрично-векторного произведенияAxˆ=\n",
    "    Ax+αAd. Если далее одна из процедурfunc,grad,hess,func_directional,grad_directional\n",
    "    вызывается в точкеxˆ, то соответствующее матрично-векторное произведениеAˆxзаново не вы-\n",
    "    числяется.\n",
    " -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c68f2a",
   "metadata": {},
   "source": [
    "## 3.1 Эксперимент: Траектория градиентного спуска на квадратичной функции\n",
    "\n",
    "Проанализируйте траекторию градиентного спуска для нескольких квадратичных функций: при-\n",
    "думайте две-три квадратичныедвумерныефункции, на которых работа метода будет отличаться, на-\n",
    "рисуйте графики с линиями уровня функций и траекториями методов.  \n",
    "\n",
    "Попробуйте ответить на следующий вопрос:Как отличается поведение метода в зависимости от\n",
    "числа обусловленности функции, выбора начальной точки и стратегии выбора шага (константная\n",
    "стратегия, Армихо, Вульф)?  \n",
    "\n",
    "Для рисования линий уровня можете воспользоваться функцией `plot_levels`, а для рисования\n",
    "траекторий `plot_trajectory` из файла `plot_trajectory_2d.py`, прилагающегося к заданию.  \n",
    "Также обратите внимание, что оракул квадратичной функции `QuadraticOracle` уже реализован в\n",
    "модуле `oracles`. Он реализует функцию $f(x) = (1/2)〈Ax, x〉−〈b, x〉$, где $A∈S^n_{++}, b ∈ R^n$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cbc4000-9807-4482-aacc-17c622b70fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import LinAlgError\n",
    "import scipy\n",
    "from scipy.optimize import line_search\n",
    "from scipy.linalg import cho_factor, cho_solve\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "\n",
    "class LineSearchTool(object):\n",
    "    \"\"\"\n",
    "    Line search tool for adaptively tuning the step size of the algorithm.\n",
    "\n",
    "    method : String containing 'Wolfe', 'Armijo' or 'Constant'\n",
    "        Method of tuning step-size.\n",
    "        Must be be one of the following strings:\n",
    "            - 'Wolfe' -- enforce strong Wolfe conditions;\n",
    "            - 'Armijo\" -- adaptive Armijo rule;\n",
    "            - 'Constant' -- constant step size.\n",
    "    kwargs :\n",
    "        Additional parameters of line_search method:\n",
    "\n",
    "        If method == 'Wolfe':\n",
    "            c1, c2 : Constants for strong Wolfe conditions\n",
    "            alpha_0 : Starting point for the backtracking procedure\n",
    "                to be used in Armijo method in case of failure of Wolfe method.\n",
    "        If method == 'Armijo':\n",
    "            c1 : Constant for Armijo rule\n",
    "            alpha_0 : Starting point for the backtracking procedure.\n",
    "        If method == 'Constant':\n",
    "            c : The step size which is returned on every step.\n",
    "    \"\"\"\n",
    "    def __init__(self, method='Wolfe', **kwargs):\n",
    "        self._method = method\n",
    "        if self._method == 'Wolfe':\n",
    "            self.c1 = kwargs.get('c1', 1e-4)\n",
    "            self.c2 = kwargs.get('c2', 0.9)\n",
    "            self.alpha_0 = kwargs.get('alpha_0', 1.0)\n",
    "        elif self._method == 'Armijo':\n",
    "            self.c1 = kwargs.get('c1', 1e-4)\n",
    "            self.alpha_0 = kwargs.get('alpha_0', 1.0)\n",
    "        elif self._method == 'Constant':\n",
    "            self.c = kwargs.get('c', 1.0)\n",
    "        else:\n",
    "            raise ValueError('Unknown method {}'.format(method))\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, options):\n",
    "        if type(options) != dict:\n",
    "            raise TypeError('LineSearchTool initializer must be of type dict')\n",
    "        return cls(**options)\n",
    "\n",
    "    def to_dict(self):\n",
    "        return self.__dict__\n",
    "\n",
    "    def line_search(self, oracle, x_k, d_k, previous_alpha=None):\n",
    "        \"\"\"\n",
    "        Finds the step size alpha for a given starting point x_k\n",
    "        and for a given search direction d_k that satisfies necessary\n",
    "        conditions for phi(alpha) = oracle.func(x_k + alpha * d_k).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        oracle : BaseSmoothOracle-descendant object\n",
    "            Oracle with .func(), .grad() methods implemented for computing\n",
    "            function values and its gradient.\n",
    "        x_k : np.array\n",
    "            Starting point\n",
    "        d_k : np.array\n",
    "            Search direction\n",
    "        previous_alpha : float or None\n",
    "            Starting point to use instead of self.alpha_0 to keep the progress from\n",
    "             previous steps. If None, self.alpha_0, is used as a starting point.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        alpha : float or None if failure\n",
    "            Chosen step size\n",
    "        \"\"\"\n",
    "        if self._method == 'Constant':\n",
    "            return self.c\n",
    "        \n",
    "        # Get initial alpha\n",
    "        alpha = previous_alpha if previous_alpha is not None else self.alpha_0\n",
    "        phi_0 = oracle.func(x_k)\n",
    "        grad_0 = oracle.grad(x_k)\n",
    "        phi_der_0 = np.dot(grad_0, d_k)\n",
    "        \n",
    "        # If gradient directional is non-negative, direction is not descent\n",
    "        if phi_der_0 >= 0:\n",
    "            return None\n",
    "        \n",
    "        if self._method == 'Wolfe':\n",
    "            try:\n",
    "                # Use scipy.optimize.line_search for Wolfe conditions\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter(\"ignore\")\n",
    "                    result = line_search(\n",
    "                        f=lambda x: oracle.func(x),\n",
    "                        myfprime=lambda x: oracle.grad(x),\n",
    "                        xk=x_k,\n",
    "                        pk=d_k,\n",
    "                        gfk=grad_0,\n",
    "                        old_fval=phi_0,\n",
    "                        c1=self.c1,\n",
    "                        c2=self.c2,\n",
    "                        amax=100\n",
    "                    )\n",
    "                \n",
    "                # scipy.optimize.line_search returns a tuple where the first element is alpha\n",
    "                alpha_wolfe = result[0]\n",
    "                \n",
    "                if alpha_wolfe is not None and alpha_wolfe > 0:\n",
    "                    return alpha_wolfe\n",
    "            except Exception as e:\n",
    "                # If Wolfe fails, we'll fall back to Armijo\n",
    "                pass\n",
    "        \n",
    "        # Armijo rule (or fallback from Wolfe)\n",
    "        rho = 0.5  # shrinkage factor for backtracking\n",
    "        alpha_curr = alpha\n",
    "        \n",
    "        while True:\n",
    "            phi_alpha = oracle.func(x_k + alpha_curr * d_k)\n",
    "            if phi_alpha <= phi_0 + self.c1 * alpha_curr * phi_der_0:\n",
    "                return alpha_curr\n",
    "            alpha_curr *= rho\n",
    "            if alpha_curr < 1e-12:  # too small step\n",
    "                return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ff1b93",
   "metadata": {},
   "source": [
    "## 3.2 Эксперимент: Зависимость числа итераций градиентного спуска от числа обусловленности и размерности пространства\n",
    "\n",
    "Исследуйте, как зависит число итераций, необходимое градиентному спуску для сходимости, от сле-\n",
    "дующих двух параметров: 1) числа обусловленности $κ ≥ 1$ оптимизируемой функции и 2) размерности\n",
    "пространства $n$ оптимизируемых переменных.  \n",
    "\n",
    "Для этого для заданных параметровnиκсгенерируйте случайным образом квадратичную задачу\n",
    "размераnс числом обусловленностиκи запустите на ней градиентный спуск с некоторой фиксиро-\n",
    "ванной требуемой точностью. Замерьте число итераций $T(n,κ)$, которое потребовалось сделать методу до сходимости (успешному выходу по критерию остановки).  \n",
    "\n",
    "**Рекомендация:** Проще всего сгенерировать случайную квадратичную задачу размера $n$ с заданным числом обусловленности $κ$ следующим образом. В качестве матрицы $A∈S^n_{++}$ удобно взять просто диагональную матрицу $A= Diag(a)$, у которой диагональные элементы сгенерированы случайно\n",
    "в пределах $[1,κ]$, причем $min(a) = 1, max(a) = κ$. В качестве вектора $b∈R^n$ можно взять вектор со случайными элементами. Диагональные матрицы удобно рассматривать, поскольку с ними можно эффективно работать даже при больших значениях $n$. Рекомендуется хранить матрицу $A$ в формате разреженной диагональной матрицы (см. `scipy.sparse.diags`).  \n",
    "\n",
    "Зафиксируйте некоторое значение размерности $n$. Переберите различные числа обусловленности\n",
    "$κ$ по сетке и постройте график зависимости $T(κ,n)$ против $κ$. Поскольку каждый раз квадратичная задача генерируется случайным образом, то повторите этот эксперимент несколько раз. В результате для фиксированного значения $n$ у Вас должно получиться целое семейство кривых зависимости $T(κ,n)$ от $κ$. Нарисуйте все эти кривые одним и тем же цветом для наглядности (например, красным).  \n",
    "\n",
    "Теперь увеличьте значение $n$ и повторите эксперимент снова. Вы должны получить новое семейство\n",
    "кривых $T(n′,κ)$ против $κ$. Нарисуйте их все одним и тем же цветом, но отличным от предыдущего\n",
    "(например, синим).  \n",
    "\n",
    "Повторите эту процедуру несколько раз для других значений $n$. В итоге должно получиться несколько разных семейств кривых - часть красных (соответствующих одному значению $n$), часть синих (соответствующих другому значению $n$), часть зеленых и т. д.  \n",
    "\n",
    "Обратите внимание, что значения размерности $n$ имеет смысл перебирать по логарифмической\n",
    "сетке (например, $n = 10, n = 100, n = 1000$ и т. д.).  \n",
    "\n",
    "Какие выводы можно сделать из полученной картинки?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d8d56a",
   "metadata": {},
   "source": [
    "## 3.3 Эксперимент: Сравнение методов градиентного спуска и Ньютона на реальной задаче логистической регрессии\n",
    "\n",
    "Сравнить методы градиентного спуска и Ньютона на задаче обучения логистической регрессии на\n",
    "реальных данных.\n",
    "\n",
    "В качестве реальных данных используйте следующие три набора с сайта LIBSVM [http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/.](http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/.)\n",
    ": *w8a*, *gisette* и *real-sim*. Коэффициент регуляризации взять стандартным образом: $λ = 1/m$.\n",
    "Параметры обоих методов взять равными параметрам по умолчанию. Начальную точку выбрать\n",
    "$x_0 = 0$.\n",
    "\n",
    "Построить графики сходимости следующих двух видов:  \n",
    "1) Зависимость значения функции от реального времени работы метода.  \n",
    "2) Зависимость относительного квадрата нормы градиента $‖∇f(x_k)‖^2_2 /‖∇f(x_0)‖^2_2$ (в логарифмической шкале) против реального времени работы.\n",
    "\n",
    "При этом оба метода (градиентный спуск и Ньютон) нужно рисовать на одном и том же графике.\n",
    "Укажите в отчете, какова стоимость итерации и сколько памяти требуется каждому из методов в\n",
    "зависимости от параметров $m$ (размер выборки) и $n$ (размерность пространства). При оценке используйте нотацию $O(·)$, скрывающую внутри себя абсолютные константы.\n",
    "\n",
    "Какие выводы можно сделать по результатам этого эксперимента? Какой из методов лучше и в\n",
    "каких ситуациях?\n",
    "\n",
    "**Рекомендация:** Любой набор данных с сайта LIBSVM представляет из себя текстовый файл в фор-\n",
    "мате svmlight. Чтобы считать такой текстовый файл, можно использовать функцию `load_svmlight_file` из модуля `sklearn.datasets`. Обратите внимание, что эта функция возвращает матрицу в формате `scipy.sparse.csr_matrix`, поэтому Ваша реализация логистического оракула должна поддерживать такие матрицы."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1936edb3",
   "metadata": {},
   "source": [
    "<!-- ## 3.4 (Бонусная часть) Эксперимент: Оптимизация вычислений в градиентном спуске\n",
    "\n",
    "Сравнить градиентный спуск на логистической регрессии для обычного оракула и оптимизирован-\n",
    "ного.\n",
    "В качестве выборки использовать модельную с размерамиm= 10000,n= 8000. Пример генерации\n",
    "модельной выборки из стандартного нормального распределения:\n",
    "\n",
    "np.random.seed(31415)\n",
    "m, n = 10000, 8000\n",
    "A = np.random.randn(m, n)\n",
    "b = np.sign(np.random.randn(m))\n",
    "\n",
    "Коэффициент регуляризации выбрать стандартнымλ= 1/m.\n",
    "Параметры метода взять равными параметрам по умолчанию. Начальную точку выбратьx 0 = 0.\n",
    "Нарисовать графики:\n",
    "\n",
    "```\n",
    "(a) Зависимость значения функции от номера итерации.\n",
    "```\n",
    "```\n",
    "(b) Зависимость значения функции от реального времени работы метода.\n",
    "```\n",
    "```\n",
    "(c) Зависимость относительного квадрата нормы градиента‖∇f(xk)‖^22 /‖∇f(x 0 )‖^22 (в логарифмиче-\n",
    "ской шкале) против реального времени работы.\n",
    "```\n",
    "При этом оба метода (с обычным оракулом и с оптимизированным) нужно рисовать на одном и том\n",
    "же графике.\n",
    "Объясните, почему траектории обоих методов на первом графике совпадают.\n",
    "\n",
    "\n",
    " -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47aed4b",
   "metadata": {},
   "source": [
    "<!-- \n",
    "## 3.5 (Бонусная часть) Эксперимент: Стратегия выбора длины шага в градиентном спуске\n",
    "\n",
    "Исследовать, как зависит поведение метода от стратегии подбора шага: константный шаг (попро-\n",
    "бовать различные значения), бэктрэкинг (попробовать различные константыc), условия Вульфа (по-\n",
    "пробовать различные параметрыc 2 ).\n",
    "Рассмотрите квадратичную функцию и логистическую регрессию с модельными данным (сгенери-\n",
    "рованными случайно).\n",
    "Запустите для этих функций градиентный спуск с разными стратегиями выбора шагаиз одной и\n",
    "той же начальной точки.\n",
    "Нарисуйте кривые сходимости (относительная невязка по функции в логарифмической шкале про-\n",
    "тив числа итераций – для квадратичной функции, относительный квадрат нормы градиента в лога-\n",
    "рифмической шкале против числа итераций – для логистической регрессии) для разных стратегий на\n",
    "одномграфике.\n",
    "Попробуйте разные начальные точки. Ответьте на вопрос:Какая стратегия выбора шага является\n",
    "самой лучшей?\n",
    "\n",
    " -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8cefe9",
   "metadata": {},
   "source": [
    "# 4 Оформление задания\n",
    "\n",
    "Результатом выполнения задания являются  \n",
    "1) Файлы `optimization.py` и `oracles.py` с реализованными методами и оракулами.  \n",
    "2) Полные исходные коды для проведения экспериментов и рисования всех графиков. Все результаты должны быть воспроизводимыми. Если вы используете случайность - зафиксируйте `seed`.  \n",
    "3) Отчет в формате `.ipynb` о проведенных исследованиях.  \n",
    "\n",
    "Каждый проведенный эксперимент следует оформить в виде отчёта в виде одного `.ipynb` документа (название раздела - название соответствующего эксперимента). Для каждого эксперимента необходимо\n",
    "сначала написать его описание: какие функции оптимизируются, каким образом генерируются данные,\n",
    "какие методы и с какими параметрами используются. Далее должны быть представлены результаты\n",
    "соответствующего эксперимента - графики, таблицы и т. д. Наконец, после результатов эксперимента\n",
    "должны быть написаны Ваши выводы - какая зависимость наблюдается и почему.\n",
    "\n",
    "**Важно:** Отчет не должен содержать минимум кода. Каждый график должен быть прокомментирован - что на нем изображено, какие выводы можно сделать из этого эксперимента. Обязательно\n",
    "должны быть подписаны оси. Если на графике нарисовано несколько кривых, то должна быть легенда.\n",
    "Сами линии следует рисовать достаточно толстыми, чтобы они были хорошо видимыми."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ae40d9",
   "metadata": {},
   "source": [
    "# 5 Проверка задания\n",
    "\n",
    "Перед отправкой задания обязательно убедитесь, что Ваша реализация проходит автоматические\n",
    "предварительныетесты `presubmit_tests.py`, выданные вместе с заданием. Для этого запустите следующую команду:\n",
    "```\n",
    ">>> nosetests3 presubmit_tests.py\n",
    "```\n",
    "\n",
    "<!-- (b) Для бонусной части (проверяются как базовые, так и бонусные тесты):\n",
    "nosetests3 presubmit_tests.py -a ’bonus’\n",
    " -->\n",
    "\n",
    "**Важно:** Решения, которые не будут проходить тесты `presubmit_tests.py`, будут автоматически\n",
    "оценены в **0 баллов**. Проверяющий не будет разбираться, почему Ваш код не работает и читать Ваш\n",
    "отчет.\n",
    "Оценка за задание будет складываться из двух частей:\n",
    "\n",
    "1) Правильность и эффективность реализованного кода.\n",
    "2) Качество отчета\n",
    "\n",
    "Правильность и эффективность реализованного кода будет оцениваться автоматически с помощью\n",
    "независимых тестов (отличных от предварительных тестов). Качество отчета будет оцениваться про-\n",
    "веряющим. При этом оценка может быть субъективной и аппеляции не подлежит.\n",
    "\n",
    "За реализацию модификаций алгоритмов и хорошие дополнительные эксперименты могут быть\n",
    "начислены дополнительные баллы. Начисление этих баллов является субъективным и безапелляцион-\n",
    "ным.\n",
    "\n",
    "**Важно:** Практическое задание выполняется самостоятельно. Если вы получили ценные советы (по\n",
    "реализации или проведению экспериментов) от другого студента, то об этом должно быть явно напи-\n",
    "сано в отчёте. В противном случае \"похожие\" решения считаются плагиатом и все задействованные\n",
    "студенты (в том числе те, у кого списали) будут сурово наказаны.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f765736",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
